Does it work:
    ✔ LIG for a single layer for single input @done(21-07-18 12:56)
    ✔ LIG for all layers for single input @done(21-07-18 12:56)
    ✔ Gradients from a single example from a single layer @done(21-07-18 12:56)
    ✔ Gradients from a single example from all layers @done(21-07-18 12:56)
    ✔ Test LIG and gradients with a batch of data @done(21-07-27 14:11)
    ✔ Implement cosine similarity between grads and attributions @done(21-07-27 14:11)
    ✔ Test using raw inputs not embeddings (for attention masks) @done(21-07-27 20:08)
    ✔ Implement a batch loader @started(21-07-27 14:13) @done(21-07-27 20:08) @lasted(5h55m13s)
    ✔ Get a working full run-through with layer_integrated_gradients and cos similarity @done(21-10-06 17:06)
    ✔ Run a test using glue/SST2! @done(21-10-06 17:06)
    ✔ What do -ve correlations mean? @done(21-10-08 11:50) [ These are fine. The grads can be +ve and -ve, while attr are always +ve.]
    ✔ Plot embedding score vs attribution score @done(21-10-08 11:50) [ NO CORRELATION. BUG HUNTING TIME ]
    ✔ Run for larger # of test examples, find train examples' mean similarity and # of top 10s as a sanity check @done(21-10-22 10:22)
    ✔ Add uuids to dataset @done(21-11-24 10:49)
    ✔ Implement fast layerwise gradients for batches of data @done(21-11-24 12:14)
    ✔ Calc all grads and save @done(21-11-24 12:14)
    ✔ Refactor cos similarities @done(21-11-24 16:58)
    ✔ Regenerate results @done(21-11-24 16:58)
    ✔ Investigate key vector having zero grad @done(21-11-24 18:56) [bleh]
    ✔ CRITICAL: You've been looking at biases only. HAVE to use compute_layer_grads_and_eval @done(21-11-25 09:30)
        BUT: you can hand it a list of layers, which makes things WAY faster. Should check if this is possible
        with LIGs too. Go fix the grads code and continue as normal.
    ✔ Function to return simils for single example @done(21-11-26 10:54)
    ✔ Function to print best matches @done(21-11-26 10:54)
    ✔ Compute simils for large # of examples @done(21-11-26 10:54)
    ✔ Re-generate histograms @done(21-11-26 11:32)
    ✔ Sentence length vs Mean score @done(21-11-26 16:23)
    ☐ Word occurence vs Mean score?
    ✔ Re-generate embedding plots @done(21-11-26 16:23)
    ✔ Gradient/attribution heatmaps @done(21-11-29 11:08)
    ☐ Per-feature correlation/contribution
    ☐ Per-label histograms
    ✔ Text examples @done(21-11-29 16:04)
    ☐ Refactor code for submission


Is it scalable:
    ☐ LIGs are already an approximation, might not need topk


Outstanding Questions:
    ☐ Are test examples with no highly correlated training examples more frequently misclassified?
    ☐ If we remove "low-quality" training examples does this reduce performance less than a random set?
    ☐ If we remove "high-quailty" training examples does this reduce performance more than a random set?


Write-up:
15-20  pgs background (+ analysis)
understands literature
can write
effort
difficulty

build out skeleton linearly
40-60 pages
add s

FREEDOM:
    ☐ Structure in attributions and gradients
    ☐ Potential remedies
    ☐ Future Work
        ☐ Ablation Study
        ☐ Top-K/Graph Parameter activations
        ☐ Small NN to validate assumption #1
    ☐ Tidy up intro
    ☐ Lit review neurons and NNs
    ☐ Lit review Seq-2-Seq
    ☐ Lit review tidy up
        ☐ Multi-layer self-attn diagram
        ☐ DistilBERT architecture
    ☐ Results tidy up
    ☐ SUBMIT


